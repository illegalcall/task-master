# Cursor Tasks for Core Processing Features

Each task should be implemented and tested individually. Only one task is to be implemented at a time; wait for approval and commit before proceeding to the next.

## Initial Job Implementation: ParseDocument

- **Task 0.1**: Create basic ParseDocument job structure.

  - Define `ParseDocumentPayload` struct with fields:
    - **PDF document** (file path, URL, or in-memory data reference)
    - **Expected output schema** (JSON format)
    - **Description** (additional context for the LLM)
    - Optional parameters (e.g., language, parsing options)
  - Implement JSON schema validation for the payload.
  - Create sample payloads for testing (e.g., invoice PDF, resume PDF, contract PDF).
  - **Unit Test**: Verify payload serialization/deserialization works correctly.

- **Task 0.2**: Implement ParseDocument job handler.

  - Create `ParseDocumentHandler` function implementing the `JobHandlerFunc` interface.
  - Integrate a PDF parsing library to extract text from the PDF.
  - Integrate an LLM using Google Gemini (key in .env as GEMINI_API_KEY) to:
    - Convert the extracted text into the structured JSON specified by the schema.
    - Use the `description` field to provide additional context for parsing.
  - Implement error handling for invalid PDFs, schema mismatches, or LLM conversion failures.
  - **Unit Test**: Test document parsing and conversion with sample PDFs and mock LLM responses, verifying that the `description` context is utilized.
  - **Integration Test**: Verify the parsed data can be ingested into a database.

- **Task 0.3**: Add parsing tracking and reporting.
  - Implement status tracking for the document parsing lifecycle (e.g., uploaded, parsing, conversion, ingested, error).
  - Create retry logic specific to document parsing failures.
  - Add metrics for success rates, processing times, and error rates.
  - Implement webhook notifications for parsing status changes.
  - **Unit Test**: Verify tracking logic for different parsing statuses.
  - **E2E Test**: Test the full document parsing workflow, including status updates and database ingestion.

---

## API Endpoint for PDF Job Submission

- **Task 1.1**: Define API Contract and Payload Validation

  - Design the API endpoint:
    - Method: POST
    - Path: /api/jobs/parse-document
    - Request Body (JSON):
      - pdf_source: (file upload, URL, or base64-encoded data)
      - expected_schema: (JSON structure for desired output)
      - description: (optional additional parsing context)
      - options: (optional parameters like language, parsing options)
      - webhook_url: (optional, for async notifications)
  - Implement payload validation:
    - Ensure required fields are present.
    - Validate file type (.pdf) and size (e.g., max 10MB).
    - Validate JSON schema for expected_schema.
    - Validate webhook_url (if provided).
  - **Unit Test**: Validate correct/incorrect payloads and edge cases.

- **Task 1.2**: Implement API Endpoint Logic

  - Create the API handler that:
    - Accepts and validates the request.
    - Stores uploaded PDFs in a temporary location (or handles URL/base64 PDFs).
    - Generates a unique job ID for tracking.
    - Calls ParseDocumentHandler to process the job.
    - Returns a JSON response with:
      - job_id
      - status (queued)
  - **Unit Test**: Ensure the handler correctly processes valid requests and rejects invalid ones.

---

## Payload Validation

- **Task 4**: Create a robust payload validation framework.

  - Implement JSON Schema validation for payload structure
  - Add `RegisterPayloadSchema(jobType string, schema string) error`
  - Create validation function `ValidatePayload(jobType string, payload []byte) error`
  - Implement default and custom validation error messages
  - **Unit Test**: Develop tests using sample valid and invalid payloads to ensure robust validation.
  - **Property Test**: Generate random payloads to test validation edge cases.

- **Task 5**: Enhance payload validation with transformation capabilities.
  - Create a pipeline for payload normalization before validation
  - Implement versioning for backward compatibility with older payload formats
  - Add payload size limits and sanitization
  - **Unit Test**: Verify that payload transformation works correctly.
  - **Integration Test**: Test with various payload types and versions.

---

## Configurable Retry Policies

- **Task 6**: Implement comprehensive retry policy configuration.

  - Create `RetryPolicy` struct with fields:
    - `MaxRetries int`
    - `InitialBackoff time.Duration`
    - `MaxBackoff time.Duration`
    - `BackoffMultiplier float64`
    - `RetryableErrors []string`
  - Add per-job-type default retry configurations
  - Create global retry policy override capabilities
  - **Unit Test**: Write tests to verify that the configuration correctly supplies retry parameters.
  - **Integration Test**: Verify retry policies can be loaded from configuration files.

- **Task 7**: Build adaptive retry mechanism with backoff strategies.

  - Implement an exponential backoff algorithm
  - Create a jitter function to prevent thundering herd problems
  - Add custom retry decision functions via `ShouldRetry(error) bool`
  - Implement retry attempt tracking and logging
  - **Unit Test**: Write tests to verify that the retry logic correctly handles failures and backoff timing.
  - **Load Test**: Verify backoff behavior under high concurrency.

- **Task 8**: Implement retry history and analytics.
  - Store retry attempts with timestamps and error messages
  - Create API endpoint `/api/jobs/:id/retry-history`
  - Add metrics for retry success/failure rates by job type
  - Implement alerting for jobs exceeding retry thresholds
  - **Unit Test**: Verify retry history recording works correctly.
  - **E2E Test**: Test API endpoint for retry history.

---

## Job Timeout Handling

- **Task 9**: Implement multi-level timeout management.

  - Create `TimeoutConfig` struct with fields:
    - `DefaultTimeout time.Duration`
    - `MaxTimeout time.Duration`
    - `WarningThreshold time.Duration`
  - Implement per job-type timeout configurations
  - Add timeout override capability when submitting jobs
  - **Unit Test**: Create tests for timeout configuration loading and validation.
  - **Integration Test**: Verify timeout configuration is correctly applied.

- **Task 10**: Build graceful timeout handling for workers.

  - Implement context-based timeout propagation
  - Create cleanup handlers for timed-out jobs
  - Add a system for graceful shutdown of long-running tasks
  - Implement resource cleanup for interrupted jobs
  - **Unit Test**: Create tests to simulate timeout scenarios and verify that jobs are cancelled as expected.
  - **Stress Test**: Verify timeout behavior under high load conditions.

- **Task 11**: Create timeout observability and monitoring.
  - Implement timeout event logging with detailed diagnostics
  - Add metrics for job timeout frequency by type
  - Create dashboard widgets for timeout visualization
  - Implement alerting for abnormal timeout patterns
  - **Unit Test**: Verify monitoring correctly captures timeout events.
  - **E2E Test**: Test dashboard with simulated timeout scenarios.

---

## Progress Tracking

- **Task 12**: Implement fine-grained progress reporting system.

  - Create `ReportProgress(jobID string, percentage float64, message string) error`
  - Implement `GetProgress(jobID string) (JobProgress, error)`
  - Design `JobProgress` struct with fields for percentage, stage, message, and timestamp
  - Add support for stage-based progress (e.g., "Extracting: 50%", "Transforming: 20%")
  - **Unit Test**: Write tests to ensure that progress updates and retrieval function correctly.
  - **Integration Test**: Verify progress updates work end-to-end.

- **Task 13**: Build real-time progress notification system.

  - Implement a WebSocket endpoint for progress updates
  - Create a Redis pub/sub channel for progress events
  - Add progress event history with timestamp retention
  - Implement throttling for high-frequency progress updates
  - **Unit Test**: Verify pub/sub functionality for progress updates.
  - **Load Test**: Test progress reporting under high-frequency updates.

- **Task 14**: Enhance UI for progress monitoring.
  - Create API endpoint `/api/jobs/:id/progress`
  - Implement progress visualization components
  - Add ETA calculation based on progress rate
  - Design progress history view with timeline
  - **Unit Test**: Develop tests to verify the endpoint returns accurate progress data.
  - **E2E Test**: Test progress visualization with simulated jobs.

---

## Job Dependencies

- **Task 15**: Design and implement job dependency framework.

  - Create `DependencyConfig` struct with fields:
    - `DependsOn []string`
    - `DependencyType string` (e.g., "all", "any", "custom")
    - `FailureAction string` (e.g., "abort", "continue", "retry")
  - Implement dependency validation on job submission
  - Create a dependency resolution system for the job scheduler
  - Add cycle detection for circular dependencies
  - **Unit Test**: Write tests to validate dependency configuration and resolution.
  - **Integration Test**: Test dependency chains with varying conditions.

- **Task 16**: Implement advanced dependency features.

  - Create conditional dependencies (run if parent succeeded/failed)
  - Implement timeout handling for stuck dependencies
  - Add cross-queue dependencies capability
  - Create visualization for dependency graphs
  - **Unit Test**: Verify conditional dependency logic.
  - **E2E Test**: Test complex dependency scenarios.

- **Task 17**: Build dependency management APIs.
  - Create API endpoints for:
    - `/api/jobs/:id/dependencies` (GET, POST, DELETE)
    - `/api/jobs/:id/dependents` (GET)
  - Implement dependency modification for queued jobs
  - Add bulk dependency operations
  - Create dependency audit logging
  - **Unit Test**: Verify API endpoints function correctly.
  - **Integration Test**: Test dependency modification through API.

---

## Job Cancellation

- **Task 18**: Implement comprehensive cancellation system.

  - Create `CancelJob(jobID string, reason string) error`
  - Implement graceful and forced cancellation modes
  - Add per job-type cancellation handlers
  - Create cancellation propagation to dependent jobs
  - **Unit Test**: Write tests to simulate job cancellation and verify proper behavior.
  - **Integration Test**: Test cancellation across distributed workers.

- **Task 19**: Enhance cancellation with user control features.

  - Implement a cancellation permission system
  - Create API endpoint `/api/jobs/:id/cancel`
  - Add cancellation reason tracking and auditing
  - Implement cancellation webhooks for notifications
  - **Unit Test**: Verify permission checks for cancellation.
  - **E2E Test**: Test cancellation flow through the API.

- **Task 20**: Build cancellation recovery mechanisms.
  - Implement partial result saving for cancelled jobs
  - Create a system for resuming cancelled jobs
  - Add cleanup for resources from cancelled jobs
  - Implement metrics for cancellation patterns
  - **Unit Test**: Verify resource cleanup after cancellation.
  - **Integration Test**: Test job resumption after cancellation.

---

## Priority Queues

- **Task 21**: Design priority system architecture.

  - Create a `Priority` enum with levels (e.g., Low, Normal, High, Critical)
  - Implement a priority field in the job submission API
  - Add priority inheritance for dependent jobs
  - Create priority boost mechanisms for aging jobs
  - **Unit Test**: Write tests to verify that priority information is correctly stored.
  - **Integration Test**: Verify priority settings are preserved throughout the job lifecycle.

- **Task 22**: Implement priority-based scheduling.

  - Modify the Kafka producer to use partitioned topics by priority
  - Create priority-aware consumer groups
  - Implement fair scheduling to prevent starvation
  - Add dynamic priority adjustment based on wait time
  - **Unit Test**: Develop tests to ensure that jobs with higher priority are processed before lower priority ones.
  - **Load Test**: Verify priority scheduling under high queue depth.

- **Task 23**: Build priority management and monitoring.
  - Create API endpoints for priority management
  - Implement priority override for administrators
  - Add metrics for priority distribution and wait times
  - Create alerts for blocked high-priority jobs
  - **Unit Test**: Verify priority management API functions.
  - **E2E Test**: Test priority visualization and management in the UI.

---

## Dead-letter Queue

- **Task 24**: Implement a comprehensive dead-letter queue system.

  - Create a dead-letter queue in Kafka
  - Implement `MoveToDeadLetter(jobID string, reason string) error`
  - Add configurable policies for when to dead-letter
  - Create metadata enrichment for dead-lettered jobs
  - **Unit Test**: Write tests to verify that failed jobs are correctly moved to the dead-letter queue.
  - **Integration Test**: Test the end-to-end dead-letter flow.

- **Task 25**: Build dead-letter management capabilities.

  - Create API endpoints:
    - `/api/dead-letter` (GET, filtering options)
    - `/api/dead-letter/:id/replay`
    - `/api/dead-letter/:id/discard`
  - Implement batch operations for dead-letter management
  - Add replay capability with modified payloads
  - Create a notification system for dead-letter events
  - **Unit Test**: Verify dead-letter management API functions.
  - **E2E Test**: Test dead-letter UI workflows.

- **Task 26**: Enhance dead-letter analytics and diagnosis.
  - Implement root cause analysis for dead-lettered jobs
  - Create pattern detection for recurring failures
  - Add dead-letter trend analysis and visualization
  - Implement auto-categorization of failure reasons
  - **Unit Test**: Verify analytics functions correctly.
  - **Integration Test**: Test pattern detection with simulated failures.

---

## Result Storage

- **Task 27**: Design a flexible result storage architecture.

  - Create a `Result` interface with implementations:
    - `SimpleResult` (string/JSON result)
    - `BinaryResult` (file/blob result)
    - `ReferenceResult` (pointer to an external system)
  - Implement a result type negotiation system
  - Add result schema validation capabilities
  - Create a result metadata system with tags
  - **Unit Test**: Write tests to ensure the schema can accommodate the required data.
  - **Integration Test**: Test with various result types.

- **Task 28**: Implement efficient result storage backends.

  - Create PostgreSQL storage for structured results
  - Implement file/S3 storage for large results
  - Add Redis caching for frequently accessed results
  - Create result storage rotation and cleanup policies
  - **Unit Test**: Verify result storage and retrieval functions.
  - **Performance Test**: Test large result handling efficiency.

- **Task 29**: Build result management and access APIs.
  - Create API endpoints:
    - `/api/jobs/:id/result`
    - `/api/jobs/:id/result/download`
  - Implement result transformation for different formats
  - Add access control for sensitive results
  - Create result streaming for large payloads
  - **Unit Test**: Develop tests to verify result API endpoints.
  - **E2E Test**: Test result visualization and download from the UI.

---

## Web UI Integration

- **Task 30**: Design and implement a comprehensive job management UI.
  - Create a job submission form with dynamic fields based on job type
  - Implement a real-time job status dashboard with WebSocket updates
  - Add a detailed job view with progress, logs, and result visualization
  - Create admin tools for system management
  - **Unit Test**: Verify UI components render correctly.
  - **E2E Test**: Test full user workflows through the UI.

---

## Legacy System Integration

- **Task 31**: Build a legacy system integration layer.

  - Create a client library with synchronous API compatibility
  - Implement optional callback mechanisms for completion notifications
  - Add transparent conversion between sync/async models
  - Create connection pooling and throttling for legacy systems
  - **Unit Test**: Verify client library functions.
  - **Integration Test**: Test with a simulated legacy system.

- **Task 32**: Enhance legacy integration with advanced features.
  - Implement a circuit breaker pattern for system protection
  - Create graceful degradation to synchronous mode on queue failures
  - Add transparent retries for legacy system integration
  - Implement detailed logging for troubleshooting
  - **Unit Test**: Verify circuit breaker behavior.
  - **Load Test**: Test behavior under failure conditions.
